# ======================================================
# FloodWatch - Gemini AI Service
# Google Gemini API Integration for Weather Chatbot
# ======================================================

import os
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import httpx
import json

logger = logging.getLogger(__name__)

# ======================================================
# CONFIGURATION
# ======================================================

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyAg4xEPKC5hPA8hUcZ0TpN0rFXTQugSrtU")
GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1"
GEMINI_MODEL = "gemini-1.5-flash"  # Stable model - alternatives: gemini-pro, gemini-1.5-pro

# ======================================================
# SYSTEM PROMPTS
# ======================================================

WEATHER_ASSISTANT_PROMPT = """Báº¡n lÃ  trá»£ lÃ½ AI thá»i tiáº¿t chuyÃªn nghiá»‡p cá»§a á»©ng dá»¥ng FloodWatch - há»‡ thá»‘ng cáº£nh bÃ¡o ngáº­p lá»¥t TP.HCM.

## Vai trÃ² cá»§a báº¡n:
- Cung cáº¥p thÃ´ng tin thá»i tiáº¿t chÃ­nh xÃ¡c cho cÃ¡c quáº­n huyá»‡n TP.HCM
- Cáº£nh bÃ¡o mÆ°a lá»›n vÃ  nguy cÆ¡ ngáº­p lá»¥t
- ÄÆ°a ra lá»i khuyÃªn an toÃ n khi di chuyá»ƒn
- Giáº£i thÃ­ch cÃ¡c hiá»‡n tÆ°á»£ng thá»i tiáº¿t báº±ng ngÃ´n ngá»¯ dá»… hiá»ƒu

## Quy táº¯c tráº£ lá»i:
1. LuÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
2. Sá»­ dá»¥ng emoji phÃ¹ há»£p Ä‘á»ƒ trá»±c quan hÆ¡n
3. Ngáº¯n gá»n, sÃºc tÃ­ch (tá»‘i Ä‘a 200 tá»«)
4. LuÃ´n Ä‘á» cáº­p Ä‘áº¿n nguá»“n dá»¯ liá»‡u náº¿u cÃ³
5. Náº¿u khÃ´ng cháº¯c cháº¯n, hÃ£y nÃ³i rÃµ
6. Æ¯u tiÃªn an toÃ n cá»§a ngÆ°á»i dÃ¹ng

## CÃ¡c quáº­n dá»… ngáº­p á»Ÿ TP.HCM:
- Quáº­n 12, BÃ¬nh Tháº¡nh, Thá»§ Äá»©c: Ngáº­p do triá»u cÆ°á»ng vÃ  mÆ°a
- Quáº­n 8, Quáº­n 6: VÃ¹ng trÅ©ng tháº¥p
- GÃ² Váº¥p, TÃ¢n BÃ¬nh: Ngáº­p cá»¥c bá»™ khi mÆ°a to

## Format tráº£ lá»i:
- Sá»­ dá»¥ng bullet points khi liá»‡t kÃª
- Bold (**text**) cho thÃ´ng tin quan trá»ng
- ThÃªm emoji á»Ÿ Ä‘áº§u cÃ¡c má»¥c chÃ­nh
"""

# ======================================================
# MESSAGE HISTORY MANAGEMENT
# ======================================================

class ConversationManager:
    """Manage conversation history for context."""
    
    def __init__(self, max_history: int = 10):
        self.max_history = max_history
        self.conversations: Dict[str, List[Dict]] = {}
    
    def add_message(self, session_id: str, role: str, content: str):
        """Add message to conversation history."""
        if session_id not in self.conversations:
            self.conversations[session_id] = []
        
        self.conversations[session_id].append({
            "role": role,
            "parts": [{"text": content}]
        })
        
        # Giá»¯ chá»‰ max_history messages gáº§n nháº¥t
        if len(self.conversations[session_id]) > self.max_history:
            self.conversations[session_id] = self.conversations[session_id][-self.max_history:]
    
    def get_history(self, session_id: str) -> List[Dict]:
        """Get conversation history."""
        return self.conversations.get(session_id, [])
    
    def clear_history(self, session_id: str):
        """Clear conversation history."""
        if session_id in self.conversations:
            del self.conversations[session_id]

# Global conversation manager
conversation_manager = ConversationManager()

# ======================================================
# GEMINI API FUNCTIONS
# ======================================================

async def call_gemini_api(
    messages: List[Dict],
    system_instruction: str = None,
    temperature: float = 0.7,
    max_tokens: int = 1024
) -> Optional[str]:
    """Call Gemini API with messages."""
    
    url = f"{GEMINI_BASE_URL}/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    
    request_body = {
        "contents": messages,
        "generationConfig": {
            "temperature": temperature,
            "maxOutputTokens": max_tokens,
            "topP": 0.95,
            "topK": 40
        }
    }
    
    if system_instruction:
        request_body["systemInstruction"] = {
            "parts": [{"text": system_instruction}]
        }
    
    try:
        logger.info(f"Calling Gemini API: {GEMINI_MODEL}")
        logger.info(f"API Key (last 8 chars): ...{GEMINI_API_KEY[-8:]}")
        logger.info(f"Request URL: {url[:80]}...")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, json=request_body)
            logger.info(f"Raw response status: {response.status_code}")
            
            # Log response status
            logger.info(f"Gemini API response status: {response.status_code}")
            
            # Check for errors before raise_for_status
            if response.status_code != 200:
                logger.error(f"Gemini API error response: {response.text}")
                return None
            
            data = response.json()
            
            # Extract text from response
            if "candidates" in data and len(data["candidates"]) > 0:
                candidate = data["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    parts = candidate["content"]["parts"]
                    if len(parts) > 0 and "text" in parts[0]:
                        logger.info("Gemini API response received successfully")
                        return parts[0]["text"]
            
            # Check for blocked content
            if "promptFeedback" in data:
                feedback = data["promptFeedback"]
                if feedback.get("blockReason"):
                    logger.warning(f"Content blocked: {feedback.get('blockReason')}")
                    return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."
            
            logger.warning(f"Unexpected Gemini response format: {data}")
            return None
            
    except httpx.TimeoutException:
        logger.error("Gemini API timeout after 30s")
        return None
    except httpx.ConnectError as e:
        logger.error(f"Gemini API connection error (check network/DNS): {e}")
        return None
    except httpx.HTTPStatusError as e:
        logger.error(f"Gemini API HTTP error: {e.response.status_code} - {e.response.text}")
        return None
    except Exception as e:
        logger.error(f"Gemini API error: {type(e).__name__} - {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        return None

# ======================================================
# CHAT FUNCTIONS
# ======================================================

def build_weather_context(weather_data: List[Dict], flood_data: Dict = None) -> str:
    """Build context string from weather and flood data."""
    context_parts = []
    
    if weather_data:
        context_parts.append("## Dá»¯ liá»‡u thá»i tiáº¿t hiá»‡n táº¡i TP.HCM:")
        for w in weather_data[:10]:  # Limit to 10 districts
            forecast_info = ""
            if w.get("forecast"):
                rain_hours = [f for f in w["forecast"] if f.get("pop", 0) > 50]
                if rain_hours:
                    forecast_info = f" | Dá»± bÃ¡o mÆ°a: {rain_hours[0].get('pop')}% lÃºc {rain_hours[0].get('hour')}"
            
            context_parts.append(
                f"- **{w.get('location')}**: {w.get('temperature')}Â°C, "
                f"{w.get('conditionText', w.get('condition'))}, "
                f"Äá»™ áº©m: {w.get('humidity')}%, "
                f"GiÃ³: {w.get('windSpeed')} km/h"
                f"{forecast_info}"
            )
    
    if flood_data:
        context_parts.append("\n## Dá»¯ liá»‡u ngáº­p lá»¥t:")
        if flood_data.get("severe"):
            context_parts.append(f"- ðŸ”´ Ngáº­p nghiÃªm trá»ng: {flood_data.get('severe')} Ä‘iá»ƒm")
        if flood_data.get("high"):
            context_parts.append(f"- ðŸŸ  Ngáº­p cao: {flood_data.get('high')} Ä‘iá»ƒm")
        if flood_data.get("rainyDistricts"):
            context_parts.append(f"- ðŸŒ§ï¸ Quáº­n Ä‘ang mÆ°a: {', '.join(flood_data.get('rainyDistricts', []))}")
    
    context_parts.append(f"\n*Cáº­p nháº­t: {datetime.now().strftime('%H:%M %d/%m/%Y')}*")
    
    return "\n".join(context_parts)

async def chat_with_weather_ai(
    user_message: str,
    session_id: str = "default",
    weather_data: List[Dict] = None,
    flood_data: Dict = None
) -> Dict[str, Any]:
    """
    Chat vá»›i AI vá» thá»i tiáº¿t vá»›i context.
    
    Args:
        user_message: Tin nháº¯n cá»§a ngÆ°á»i dÃ¹ng
        session_id: ID session Ä‘á»ƒ tracking conversation
        weather_data: Dá»¯ liá»‡u thá»i tiáº¿t hiá»‡n táº¡i
        flood_data: Dá»¯ liá»‡u ngáº­p lá»¥t
    
    Returns:
        Dict vá»›i response vÃ  metadata
    """
    
    # Build system prompt vá»›i context
    system_prompt = WEATHER_ASSISTANT_PROMPT
    
    if weather_data or flood_data:
        context = build_weather_context(weather_data, flood_data)
        system_prompt += f"\n\n## Dá»¯ liá»‡u thá»±c táº¿ hiá»‡n táº¡i:\n{context}"
    
    # Add user message to history
    conversation_manager.add_message(session_id, "user", user_message)
    
    # Get conversation history
    messages = conversation_manager.get_history(session_id)
    
    # Call Gemini API
    response_text = await call_gemini_api(
        messages=messages,
        system_instruction=system_prompt,
        temperature=0.7,
        max_tokens=1024
    )
    
    if response_text:
        # Add assistant response to history
        conversation_manager.add_message(session_id, "model", response_text)
        
        return {
            "success": True,
            "response": response_text,
            "session_id": session_id,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    else:
        # Fallback response
        fallback = get_fallback_response(user_message)
        return {
            "success": False,
            "response": fallback,
            "session_id": session_id,
            "error": "Gemini API khÃ´ng pháº£n há»“i",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

# ======================================================
# FALLBACK RESPONSES
# ======================================================

def get_fallback_response(user_message: str) -> str:
    """Get fallback response when Gemini API fails."""
    
    message_lower = user_message.lower()
    
    if any(word in message_lower for word in ["thá»i tiáº¿t", "nhiá»‡t Ä‘á»™", "náº¯ng", "mÃ¡t"]):
        return """ðŸŒ¤ï¸ **ThÃ´ng tin thá»i tiáº¿t TP.HCM:**

Xin lá»—i, tÃ´i khÃ´ng thá»ƒ káº¿t ná»‘i vá»›i dá»¯ liá»‡u thá»i tiáº¿t lÃºc nÃ y.

Báº¡n cÃ³ thá»ƒ:
â€¢ Xem tháº» thá»i tiáº¿t bÃªn trÃ¡i Ä‘á»ƒ cáº­p nháº­t nhanh
â€¢ Thá»­ láº¡i sau vÃ i giÃ¢y
â€¢ Kiá»ƒm tra káº¿t ná»‘i internet

âš ï¸ Náº¿u báº¡n cáº§n thÃ´ng tin kháº©n cáº¥p vá» ngáº­p lá»¥t, vui lÃ²ng gá»i Ä‘Æ°á»ng dÃ¢y nÃ³ng: **1900-xxxx**"""

    if any(word in message_lower for word in ["mÆ°a", "mÆ°a to", "mÆ°a lá»›n"]):
        return """ðŸŒ§ï¸ **ThÃ´ng tin vá» mÆ°a:**

Hiá»‡n táº¡i tÃ´i khÃ´ng thá»ƒ truy cáº­p dá»¯ liá»‡u mÆ°a real-time.

**CÃ¡c quáº­n hay ngáº­p khi mÆ°a to táº¡i TP.HCM:**
â€¢ Quáº­n 12, Thá»§ Äá»©c - triá»u cÆ°á»ng + mÆ°a
â€¢ Quáº­n 8, Quáº­n 6 - vÃ¹ng trÅ©ng
â€¢ BÃ¬nh Tháº¡nh, GÃ² Váº¥p - ngáº­p cá»¥c bá»™

ðŸ’¡ **Khuyáº¿n cÃ¡o:** TrÃ¡nh di chuyá»ƒn qua vÃ¹ng ngáº­p khi mÆ°a to."""

    if any(word in message_lower for word in ["ngáº­p", "lá»¥t", "nÆ°á»›c", "ngáº­p lá»¥t"]):
        return """ðŸŒŠ **Cáº£nh bÃ¡o ngáº­p lá»¥t:**

TÃ´i khÃ´ng thá»ƒ láº¥y dá»¯ liá»‡u ngáº­p lá»¥t real-time lÃºc nÃ y.

**Biá»‡n phÃ¡p an toÃ n:**
1. ðŸš— KhÃ´ng cá»‘ lÃ¡i xe qua vÃ¹ng ngáº­p
2. ðŸ“± Theo dÃµi cáº£nh bÃ¡o tá»« á»©ng dá»¥ng
3. ðŸ  Di chuyá»ƒn Ä‘á»“ Ä‘áº¡c lÃªn cao náº¿u á»Ÿ vÃ¹ng trÅ©ng
4. ðŸ“ž LiÃªn há»‡ cá»©u há»™ náº¿u cáº§n: **114**

Vui lÃ²ng thá»­ láº¡i sau Ä‘á»ƒ xem dá»¯ liá»‡u má»›i nháº¥t."""

    return """ðŸ‘‹ Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ thá»i tiáº¿t AI cá»§a FloodWatch.

Hiá»‡n táº¡i tÃ´i gáº·p sá»± cá»‘ káº¿t ná»‘i. Báº¡n cÃ³ thá»ƒ há»i tÃ´i vá»:

â€¢ â˜€ï¸ Thá»i tiáº¿t cÃ¡c quáº­n TP.HCM
â€¢ ðŸŒ§ï¸ Dá»± bÃ¡o mÆ°a trong 5 giá» tá»›i
â€¢ ðŸŒŠ Cáº£nh bÃ¡o ngáº­p lá»¥t
â€¢ ðŸ›£ï¸ TÆ° váº¥n di chuyá»ƒn an toÃ n

Vui lÃ²ng thá»­ láº¡i cÃ¢u há»i cá»§a báº¡n!"""

# ======================================================
# QUICK ACTIONS
# ======================================================

async def get_weather_advice(weather_data: List[Dict]) -> str:
    """Get quick weather advice based on current conditions."""
    
    if not weather_data:
        return "KhÃ´ng cÃ³ dá»¯ liá»‡u thá»i tiáº¿t Ä‘á»ƒ phÃ¢n tÃ­ch."
    
    prompt = f"""Dá»±a trÃªn dá»¯ liá»‡u thá»i tiáº¿t sau, hÃ£y Ä‘Æ°a ra 3 lá»i khuyÃªn ngáº¯n gá»n (má»—i lá»i khuyÃªn 1 dÃ²ng) cho ngÆ°á»i dÃ¢n TP.HCM:

{build_weather_context(weather_data)}

Tráº£ lá»i vá»›i format:
1. [emoji] Lá»i khuyÃªn 1
2. [emoji] Lá»i khuyÃªn 2
3. [emoji] Lá»i khuyÃªn 3"""

    messages = [{"role": "user", "parts": [{"text": prompt}]}]
    
    response = await call_gemini_api(
        messages=messages,
        system_instruction="Báº¡n lÃ  chuyÃªn gia thá»i tiáº¿t. Tráº£ lá»i ngáº¯n gá»n, thá»±c táº¿.",
        temperature=0.5,
        max_tokens=300
    )
    
    return response or "KhÃ´ng thá»ƒ táº¡o lá»i khuyÃªn lÃºc nÃ y."

async def analyze_flood_risk(weather_data: List[Dict], flood_data: Dict) -> str:
    """Analyze flood risk based on weather and flood data."""
    
    prompt = f"""PhÃ¢n tÃ­ch nguy cÆ¡ ngáº­p lá»¥t dá»±a trÃªn dá»¯ liá»‡u sau:

{build_weather_context(weather_data, flood_data)}

HÃ£y Ä‘Æ°a ra:
1. ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ nguy cÆ¡ (Tháº¥p/Trung bÃ¬nh/Cao/Ráº¥t cao)
2. CÃ¡c quáº­n cáº§n chÃº Ã½
3. Khuyáº¿n cÃ¡o ngáº¯n gá»n

Format: bullet points, cÃ³ emoji."""

    messages = [{"role": "user", "parts": [{"text": prompt}]}]
    
    response = await call_gemini_api(
        messages=messages,
        system_instruction="Báº¡n lÃ  chuyÃªn gia cáº£nh bÃ¡o thiÃªn tai. Æ¯u tiÃªn an toÃ n ngÆ°á»i dÃ¢n.",
        temperature=0.3,
        max_tokens=500
    )
    
    return response or "KhÃ´ng thá»ƒ phÃ¢n tÃ­ch nguy cÆ¡ ngáº­p lÃºc nÃ y."

# ======================================================
# UTILITY FUNCTIONS
# ======================================================

def clear_session(session_id: str):
    """Clear conversation history for a session."""
    conversation_manager.clear_history(session_id)
    logger.info(f"Cleared session: {session_id}")

def get_session_info(session_id: str) -> Dict[str, Any]:
    """Get info about a conversation session."""
    history = conversation_manager.get_history(session_id)
    return {
        "session_id": session_id,
        "message_count": len(history),
        "has_history": len(history) > 0
    }
